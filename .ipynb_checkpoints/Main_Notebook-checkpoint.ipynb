{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb957f46",
   "metadata": {
    "id": "bb957f46"
   },
   "source": [
    "### Introduction\n",
    "### 1. Business Overview\n",
    "Real Estate Investment Firms provide comprehensive investment advisory services, including market research, property analysis, due diligence, financial modeling, and portfolio management.\n",
    "Our goal is to optimize investment decisions, mitigate risks, and ensure long-term success.\n",
    "\n",
    "The primary focus of this project is to identify opportunities in real estate markets and capitalize them to generate significant profits. We will carefully assess and mitigate risks associated with each investment in relation to zipcodes, based on some factors like market volatility.\n",
    "We will conduct a market analysis to identify areas of high demand and growth for optimal investment and prioritize investments with the potential substantial returns based on factors like property appreciation and market demand.\n",
    "\n",
    "With long term value our investement strategies will focus on the ability to generate consistent cashflows overtime. Real Estate Firms can achieve a long-term partnerships with clients by achieving their financial objectives through successful real estate investments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794604d4",
   "metadata": {
    "id": "794604d4"
   },
   "source": [
    "### 1. Problem Statement.\n",
    "\n",
    "At Matawi Real Estate Investment firm we seek to identify the top five zip codes for potential investment opportunities. The firm aims to maximize return on investment by strategically selecting zip codes that exhibit strong growth potential and promising real estate market conditions. By leveraging data from Zillow Research,our  firm intends to make data-driven investment decisions and optimize investment portfolio.\n",
    "\n",
    "The investment firm needs to determine the top five zip codes that present the best investment opportunities based on real estate market trends and historical data. We will conduct a comprehensive analysis of various factors, such as past price trends, growth rates, market demand, and other relevant indicators to identify zip codes with the highest potential for future price appreciation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197a933",
   "metadata": {
    "id": "9197a933"
   },
   "source": [
    "### 2. Objectives\n",
    "  main objective: \n",
    " - The main objective is to develop a forecasting model that can accurately predict real estate price movements in different zip codes and assist in identifying the most favorable locations for investment between the period of April 1996 to April 2018. \n",
    "\n",
    "specific objectives: \n",
    "-  To assess and mitigate potential risks associated with market volatility and economic fluctuations.\n",
    "- To Utilize time series analysis techniques to identify underlying patterns, trends, and seasonality in the real estate price data\n",
    "- To Build a time series predictive model that can forecast real estate prices for various zip codes\n",
    "- To Evaluate the forecasting model's performance by comparing its predictions against actual real estate prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36775271",
   "metadata": {
    "id": "36775271"
   },
   "source": [
    "# Success Metrics.\n",
    "For the success creterion we will use the *\" Root mean squared  error \"* and measure accuracy of our models. \n",
    "We shall be aiming at an RMSE value of between 0.2 and 0.5.\n",
    "This indicates that we  want the predictions to be reasonably close to the actual values, with an acceptable level of error.\n",
    "\n",
    "During the model training and evaluation process, you will calculate the RMSE for each model and compare it to your target range. Here's how we would interpret the results:\n",
    "\n",
    "If the RMSE is below 0.2: This indicates excellent performance, as the predictions have a very small average error compared to the actual values. It suggests that our  model is accurate and reliable.\n",
    "\n",
    "If the RMSE is between 0.2 and 0.5: This falls within our  target range and suggests that our model is performing well. It means that, on average, the predictions are within a reasonable distance from the actual values.\n",
    "\n",
    "If the RMSE is above 0.5: This suggests that your model's performance may not be satisfactory. The predictions have a relatively larger average error compared to the actual values. We may need to further improve your model or explore alternative approaches to achieve better accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f7ec9",
   "metadata": {
    "id": "8f8f7ec9"
   },
   "source": [
    "### 2. Data Understanding\n",
    "\n",
    "The dataset used in this project consists of historic median house prices from various regions in the USA. It covers a time period of 22 years, specifically from April 1996 to April 2018. The dataset was obtained from the [Zillow website.](https://www.zillow.com/research/data/)\n",
    "\n",
    "Here are the key details about the dataset:\n",
    "\n",
    "* It contains 14,723 rows and 272 columns.\n",
    "* Out of the 272 columns, 4 columns are categorical, while the rest are numerical.\n",
    "\n",
    "The columns are described as follows:\n",
    "> RegionID: A unique identifier for each region.\n",
    "\n",
    "> RegionName: The names of the regions, represented by zip codes.\n",
    "\n",
    "> City: The corresponding city names for each region.\n",
    "\n",
    "> State: The names of the states where the regions are located.\n",
    "\n",
    "> Metro: The names of the metropolitan areas associated with the regions.\n",
    "\n",
    "> County Name: The names of the counties where the regions are situated.\n",
    "\n",
    "> Size Rank: The ranking of the zip codes based on urbanization.\n",
    "\n",
    "> Date Columns (265 Columns): These columns represent different dates and provide median house prices for each region over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d7157",
   "metadata": {
    "id": "0f9d7157"
   },
   "source": [
    "### 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6d0e33",
   "metadata": {
    "id": "7b6d0e33"
   },
   "outputs": [],
   "source": [
    "# importing the Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5963a3b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "5963a3b0",
    "outputId": "b44d185c-87d4-422c-a78d-904634ef5aed"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/zillow_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15215/3144508445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Previewing the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/zillow_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/zillow_data.csv'"
     ]
    }
   ],
   "source": [
    "# Previewing the dataset.\n",
    "df = pd.read_csv('/content/zillow_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c88af",
   "metadata": {
    "id": "f70c88af"
   },
   "outputs": [],
   "source": [
    "# A function to analyze the shape, number of columns, and information of the dataset\n",
    "def analyze_dataset(df):\n",
    "    \"\"\"\n",
    "    This function  outputs information about the shape,\n",
    "    columns, and information of the dataset using the Pandas library.\n",
    "    \"\"\"\n",
    "    # Output the shape of the dataset\n",
    "    print(\"Shape of dataset:\", df.shape)\n",
    "    print('\\n-----------------------------------------------------------')\n",
    "\n",
    "    # Output the column names of the dataset\n",
    "    print(\"Column names:\", list(df.columns))\n",
    "    print('\\n-----------------------------------------------------------')\n",
    "\n",
    "    # Output information about the dataset\n",
    "    print(df.info())\n",
    "    print('\\n-----------------------------------------------------------')\n",
    "\n",
    "    # output descriptive statistics about the dataset\n",
    "    print(df.describe())\n",
    "    print('\\n-----------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a6860",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d9a6860",
    "outputId": "f51c7616-e352-409f-984c-30b52241f31e"
   },
   "outputs": [],
   "source": [
    "analyze_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d53d08",
   "metadata": {
    "id": "13d53d08"
   },
   "source": [
    "The dataset has 14723 rows and 272 columns,4 categorical and the rest are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520e191",
   "metadata": {
    "id": "f520e191"
   },
   "outputs": [],
   "source": [
    "#Checking for duplicates and missing data\n",
    "def cleaning(data):\n",
    "    \"This is a simple function to get missing and duplicated values\"\n",
    "    missing = data.isna().sum().sum()\n",
    "    duplicated = data.duplicated().sum()\n",
    "    return (f\"There are '{missing}' missing values and '{duplicated}' duplicated values in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98087d17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "98087d17",
    "outputId": "7dda6c41-fbd5-47a7-a59a-3f273ec03450"
   },
   "outputs": [],
   "source": [
    "cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba694c77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ba694c77",
    "outputId": "ad6a232a-7387-4cf4-b53a-28c84a7d0a67"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe to display datatypes and, the unique values.\n",
    "desc = []\n",
    "for i in df.columns:\n",
    "    desc.append([\n",
    "        i,\n",
    "        df[i].dtypes,\n",
    "        df[i].nunique(),\n",
    "    ])\n",
    "\n",
    "pd.DataFrame(data = desc, columns=['Feature','Dtypes','Sample_Unique'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a701b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "784a701b",
    "outputId": "d66d78c9-b1c3-42ee-c15c-119250eecdd3"
   },
   "outputs": [],
   "source": [
    "def missing_values_percentage(df):\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    total_cells = df.size\n",
    "    percentage_missing = (total_missing / total_cells) * 100\n",
    "    return percentage_missing\n",
    "\n",
    "missing_values_percentage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b72d9",
   "metadata": {
    "id": "798b72d9"
   },
   "source": [
    "The missing values are 3.94% of the entire dataset.Let's preview the percentage of the missing values per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3fb48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32b3fb48",
    "outputId": "c2b3cab2-1c69-4e06-d2f2-b0454a37cad8"
   },
   "outputs": [],
   "source": [
    "missing_values = df.isnull().mean() * 100\n",
    "\n",
    "# Print the list of columns in the DataFrame along with their missing percentages\n",
    "for column in missing_values.index:\n",
    "    print(column, missing_values[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83614c99",
   "metadata": {
    "id": "83614c99"
   },
   "source": [
    "The percentage of the missing values per column is still low ranging from 1%-7% thus we chose to fill the missing values for the metro column with missing then dropping the missing values in the date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350eb232",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "350eb232",
    "outputId": "e594075f-55e8-482d-d71b-7607a15cbb16"
   },
   "outputs": [],
   "source": [
    "## Fill the `metro` column with the word \"missing\"\n",
    "df['Metro'].fillna('missing', inplace=True)\n",
    "\n",
    "## Handling the date columns' missing values\n",
    "df.dropna(inplace=True)\n",
    "missing_values_percentage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de73af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88de73af",
    "outputId": "f2f6c8f4-157f-491e-9a97-819c784fd6ca"
   },
   "outputs": [],
   "source": [
    "print(missing_values_percentage(df))\n",
    "print(cleaning(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76669f9",
   "metadata": {
    "id": "b76669f9"
   },
   "source": [
    "The dataset doesn't have any missing values or any duplicates. Since region ID is the unique identifier, let's check if there is any duplicates in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5250f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "6d5250f7",
    "outputId": "033002e2-25ee-45e2-a549-8c07ec5edf48"
   },
   "outputs": [],
   "source": [
    "df[df['RegionID'].duplicated(keep=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c4041",
   "metadata": {
    "id": "e86c4041"
   },
   "source": [
    "The data doesn't have any duplicated ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8260d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55c8260d",
    "outputId": "a8fc9824-4d74-4ce5-d7a9-d7b51acaf6b2"
   },
   "outputs": [],
   "source": [
    "def check_value_counts(data):\n",
    "    for column in data.columns:\n",
    "        print(f'value counts for {column}')\n",
    "        print(data[column].value_counts())\n",
    "        print('------------------------------------------','\\n')\n",
    "\n",
    "check_value_counts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c6d08",
   "metadata": {
    "id": "456c6d08"
   },
   "source": [
    "The data doesn't have any data inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e5f4e",
   "metadata": {
    "id": "e03e5f4e"
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d910e",
   "metadata": {
    "id": "991d910e"
   },
   "outputs": [],
   "source": [
    "#rename RegionName column to Zipcode\n",
    "df.rename(columns={'RegionName':'ZipCode'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f497d38",
   "metadata": {
    "id": "6f497d38"
   },
   "outputs": [],
   "source": [
    "#convert Zipcode column values to string\n",
    "df.ZipCode = df.ZipCode.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545c03a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d545c03a",
    "outputId": "658dd5fc-d524-4611-9e17-66e07d90dfa7"
   },
   "outputs": [],
   "source": [
    "print(df.ZipCode.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae90ad4",
   "metadata": {
    "id": "aae90ad4"
   },
   "outputs": [],
   "source": [
    "# The zipcodes need to be 5 digits long, so a zero will be added to the ones that have four digits\n",
    "df['ZipCode'] = df['ZipCode'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf13d1",
   "metadata": {
    "id": "cfaf13d1"
   },
   "source": [
    "\n",
    "In order to address the issues identified in the business understanding phase, two new columns will be generated: one for calculating the return on investment (ROI) and another for determining the coefficient of variation. The coefficient of variation measures the extent of data point dispersion around the mean and indicates the ratio of standard deviation to the mean. This enables investors to evaluate the level of risk involved relative to the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754d60a",
   "metadata": {
    "id": "2754d60a"
   },
   "outputs": [],
   "source": [
    "# Calculating and creating a new column - ROI\n",
    "df['ROI'] = (df['2018-04'] / df['1996-04']) - 1\n",
    "\n",
    "# Calculating standard deviation (std) to be used for CV\n",
    "df[\"std\"] = df.loc[:, \"1996-04\":\"2018-04\"].std(skipna=True, axis=1)\n",
    "\n",
    "# Calculating mean to be used for CV\n",
    "df[\"mean\"] = df.loc[:, \"1996-04\":\"2018-04\"].mean(skipna=True, axis=1)\n",
    "\n",
    "# Calculating and creating a new column - CV\n",
    "df[\"CV\"] = df['std'] / df[\"mean\"]\n",
    "\n",
    "# Dropping std and mean columns as they are not necessary for analysis\n",
    "df.drop([\"std\", \"mean\"], inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324961c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "d324961c",
    "outputId": "9ff9c942-0b5e-49cf-cdfb-e0c4092e48c4"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187d3e3",
   "metadata": {
    "id": "3187d3e3"
   },
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca5fa9",
   "metadata": {
    "id": "b1ca5fa9"
   },
   "outputs": [],
   "source": [
    "melted_df = df.copy()# creating a copy of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701cf172",
   "metadata": {
    "id": "701cf172"
   },
   "source": [
    "The original dataset has 265 datetime columns which makes it challenging to do any data analysis and visualization. We'll melt the dataframe so that the dates are in one column and have the values in one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d6f73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "857d6f73",
    "outputId": "2d9ed7dd-a497-4634-cbd9-3baee28a689c"
   },
   "outputs": [],
   "source": [
    "def melt_data(df):\n",
    "\n",
    "    melted = pd.melt(df, id_vars=['ZipCode', 'RegionID', 'SizeRank', 'City', 'State', 'Metro', 'CountyName','ROI','CV'], var_name='time')\n",
    "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True)\n",
    "    melted = melted.dropna(subset=['value'])\n",
    "    return melted #.groupby('time').aggregate({'value':'mean'})\n",
    "\n",
    "melted_df = melt_data(melted_df)\n",
    "melted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c7fb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "903c7fb6",
    "outputId": "b068fcbd-5efa-4b26-8ff5-090341bf14bf"
   },
   "outputs": [],
   "source": [
    "analyze_dataset(melted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9951f",
   "metadata": {
    "id": "04a9951f"
   },
   "source": [
    "The new dataset has 3417175 rows and 11 columns.The data is from 4th April 1996 to 4th April 2018.The house with the lowest price has a price of 11400 dollars and the one with the highest price has a price of 8558700 dollars. The highest ROI on a house is 11.2% and the lowest ROI on a house is -53.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ac5c9",
   "metadata": {
    "id": "9b3ac5c9"
   },
   "source": [
    "### 4.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d2835",
   "metadata": {
    "id": "440d2835"
   },
   "outputs": [],
   "source": [
    "def plot_value_counts(data, columns, top_n=20):\n",
    "    \"\"\"\n",
    "    Plots bar plots of value counts for the specified columns in the given dataset,\n",
    "    considering only the top_n items.\n",
    "    Parameters:\n",
    "    data (DataFrame): The dataset to analyze.\n",
    "    columns (list): List of column names to plot value counts for.\n",
    "    top_n (int): Number of top items to consider (default: 20).\n",
    "    \"\"\"\n",
    "\n",
    "    num_plots = len(columns)\n",
    "    num_rows = 2\n",
    "    num_cols = 2\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "    fig.tight_layout()\n",
    "    for i, column in enumerate(columns):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axes[row, col]\n",
    "        value_counts = data[column].value_counts().head(top_n)\n",
    "        sns.barplot(y=value_counts.index, x=value_counts.values, ax=ax)\n",
    "        ax.set_title(f'Top {top_n} Value Counts of {column}')\n",
    "        ax.set_xlabel(column)\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    # Hide empty subplots if there are any\n",
    "    if num_plots < num_rows * num_cols:\n",
    "        for i in range(num_plots, num_rows * num_cols):\n",
    "            row = i // num_cols\n",
    "            col = i % num_cols\n",
    "            fig.delaxes(axes[row, col])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df50a1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "5df50a1f",
    "outputId": "ad975d7b-710f-4c59-c12a-d1bc927e9849"
   },
   "outputs": [],
   "source": [
    "columns_list = [\"City\",\"State\",\"Metro\",\"CountyName\"]\n",
    "plot_value_counts(melted_df, columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lSHaQszMVqWk",
   "metadata": {
    "id": "lSHaQszMVqWk"
   },
   "source": [
    "The top 5 cities, states metro and counties with the highest number of houses are:\n",
    "* cities: New York, Los Angeles, Houston, San Antonio and Washington\n",
    "* states: CA, NY, TX,PA,FL\n",
    "* metro: New York, Los Angeles, Chicago, Philadelphia, Washington\n",
    "* counties: Los Angeles,Jefferson, Orange, Washington, Montgomery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81a40e",
   "metadata": {
    "id": "5e81a40e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_column_distributions(data, columns):\n",
    "    num_columns = len(columns)\n",
    "    fig, axes = plt.subplots(num_columns, 2, figsize=(10*2, 6*num_columns))\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        ax1 = axes[i, 0]\n",
    "        ax2 = axes[i, 1]\n",
    "\n",
    "        # Plot histogram using seaborn\n",
    "        sns.histplot(data[column], ax=ax1, bins=30, kde=False, edgecolor='black')\n",
    "        ax1.set_title(f'{column} Distribution (Histogram)', fontsize=16)\n",
    "        ax1.set_xlabel(column, fontsize=12)\n",
    "        ax1.set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "        # Plot kernel density plot using seaborn\n",
    "        sns.kdeplot(data[column], ax=ax2, fill=True)\n",
    "        ax2.set_title(f'{column} Distribution (Kernel Density Plot)', fontsize=16)\n",
    "        ax2.set_xlabel(column, fontsize=12)\n",
    "        ax2.set_ylabel('Density', fontsize=12)\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f777d19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "id": "2f777d19",
    "outputId": "50a9f075-389d-4cdb-937e-39a41214046f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "continuous_columns = ['ROI','CV','value']\n",
    "plot_column_distributions(melted_df, continuous_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_QGa1uEoWlfR",
   "metadata": {
    "id": "_QGa1uEoWlfR"
   },
   "source": [
    "> ROI: The distribution is positively skewed.Most of the houses have an ROI between 1% and 2% .It also has a long tail showing that there are outliers, houses with higher ROI impliying higher return.\n",
    "\n",
    ">Value: The distribution of the house prices is positively skewed showing that most houses are lowly priced and it also has a long tail showing that there are outliers ie the extremely highly priced houses.\n",
    "\n",
    "> CV:  The plot shows that most of the houses have a cv between 0.1 and 0.3 which shows that their prices are close to the mean thus less risk but it also has a long tail showing that there are outliers, houses with higher cv impliying higher risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8832b55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "d8832b55",
    "outputId": "f215c68d-5672-4368-d27c-7755a72377de"
   },
   "outputs": [],
   "source": [
    "def check_outliers(data, columns):\n",
    "    fig, axes = plt.subplots(nrows=len(columns), ncols=1, figsize=(20,10))\n",
    "    for i, column in enumerate(columns):\n",
    "        # Use interquartile range (IQR) to find outliers for the specified column\n",
    "        q1 = data[column].quantile(0.25)\n",
    "        q3 = data[column].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        print(\"IQR for {} column: {}\".format(column, iqr))        # Determine the outliers based on the IQR\n",
    "        outliers = (data[column] < q1 - 1.5 * iqr) | (data[column] > q3 + 1.5 * iqr)\n",
    "        print(\"Number of outliers in {} column: {}\".format(column, outliers.sum()))        # Create a box plot to visualize the distribution of the specified column\n",
    "        sns.boxplot(data=data, x=column, ax=axes[i])\n",
    "plt.show()\n",
    "\n",
    "num=melted_df.select_dtypes('number')\n",
    "columns=num.columns\n",
    "check_outliers(melted_df, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ilR4_zIPZC6L",
   "metadata": {
    "id": "ilR4_zIPZC6L"
   },
   "source": [
    "The box plots shows that there are outliers in the dataset especially in the prices(value) column which shows there are some houses that are highly priced which might provide useful information for the analysis, thus we won't remove the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682af792",
   "metadata": {
    "id": "682af792"
   },
   "source": [
    "### 4.2 Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c55cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "182c55cd",
    "outputId": "45d53f8d-ab2b-486c-cdc6-b96debd5643d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scatter_plot(x,y, x_label, y_label):\n",
    "  plt.scatter(x,y)\n",
    "  plt.xlabel(x_label)\n",
    "  plt.ylabel(y_label)\n",
    "  plt.title(f'Relationship between {x_label} and {y_label}')\n",
    "  plt.show();\n",
    "\n",
    "scatter_plot(melted_df['ROI'],melted_df['CV'],'ROI','CV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5XG0pl8Zo2q",
   "metadata": {
    "id": "t5XG0pl8Zo2q"
   },
   "source": [
    "The plot shows the relationship between the return on investment and the coefficient of variation. It shows that the two have a strong positive relationship, that is, that increase in CV leads to increase in ROI and vice versa. This implies that the higher the risk, the higher the return.\n",
    "\n",
    "Since the two have such a strong relationship, findings using ROI will be similar to those using CV . Let's examine how the other variables are related to ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f84971",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "17f84971",
    "outputId": "a154b527-f400-4dcc-9115-fe104479786b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_bivariate_analysis(data, x_column, y_column, top_n=20):\n",
    "\n",
    "    top_categories = data[x_column].value_counts().nlargest(top_n).index\n",
    "    data_top = data[data[x_column].isin(top_categories)]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for category in top_categories:\n",
    "        category_data = data_top[data_top[x_column] == category]\n",
    "        plt.scatter(category_data[x_column], category_data[y_column], label=category)\n",
    "\n",
    "    plt.title(f'Bivariate Analysis: {x_column} vs {y_column}')\n",
    "    plt.xlabel(x_column)\n",
    "    plt.ylabel(y_column)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(title=x_column)\n",
    "plt.show()\n",
    "plot_bivariate_analysis(melted_df, 'State', 'ROI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A4hlATgXbDY_",
   "metadata": {
    "id": "A4hlATgXbDY_"
   },
   "source": [
    "The above plot shows that the state with the highest return on investment is NY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfeaceb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "id": "9cfeaceb",
    "outputId": "10ed9788-97d2-484f-b474-bda97a7c6d10",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_bivariate_analysis(melted_df, 'CountyName', 'ROI')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43Zfdj38bZS2",
   "metadata": {
    "id": "43Zfdj38bZS2"
   },
   "source": [
    "The county with the highest ROI is Suffolk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e3161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "id": "2f5e3161",
    "outputId": "794317cf-dd2e-46fc-f7d0-b3dd20abd5bc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_bivariate_analysis(melted_df, 'Metro', 'ROI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xx1tt1a7blJz",
   "metadata": {
    "id": "xx1tt1a7blJz"
   },
   "source": [
    "The metro with the highest ROI is New york."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc0bd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "afbc0bd7",
    "outputId": "b67e8770-e862-4e53-9526-0833b7720ead"
   },
   "outputs": [],
   "source": [
    "plot_bivariate_analysis(melted_df, 'City', 'ROI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aOisdS_Ybvst",
   "metadata": {
    "id": "aOisdS_Ybvst"
   },
   "source": [
    "The city with the highest ROI is NewYork. From the above analysis, we can conclude that properties in NewYork have the highest  return on investment.\n",
    "Let's analyse the cities, states, metro and counties that have the highest ROI(return) but lowest CV(risk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e21c70",
   "metadata": {
    "id": "68e21c70"
   },
   "outputs": [],
   "source": [
    "def get_top_rows(data, cv_column, roi_column, value_column, num_rows=10000):\n",
    "    # Sort the DataFrame based on the value column in descending order,\n",
    "    # coefficient of variance column in ascending order,\n",
    "    # and return on investment column in descending order\n",
    "    sorted_data = data.sort_values([value_column, cv_column, roi_column], ascending=[False, True, False])\n",
    "    # Get the top N rows\n",
    "    top_rows = sorted_data.head(num_rows)\n",
    "    return top_rows# Usage example\n",
    "top_rows = get_top_rows(melted_df, 'CV', 'ROI', 'value', num_rows=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68072485",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "68072485",
    "outputId": "b324c36c-0c53-4ede-f534-a966ec28fa85"
   },
   "outputs": [],
   "source": [
    "top_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb8c83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "8ecb8c83",
    "outputId": "7796df19-5355-4c11-de87-bece1f05822d"
   },
   "outputs": [],
   "source": [
    "plot_value_counts(top_rows, columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WT3mzy6ccTpu",
   "metadata": {
    "id": "WT3mzy6ccTpu"
   },
   "source": [
    "The top 5 cities, states metro and counties with the highest return and lowest risk are:\n",
    "\n",
    "* cities: San Fransisco, Los Angeles, Newport beach, Beverly Hills and Rancho Santa Fe\n",
    "* states: CA, NY,FL, CO, NJ\n",
    "* metro: Los Angeles,San Fransisco,New York, San Jose, San Diego\n",
    "* counties: Los Angeles,Santa Clara, San Fransisco, Orange, San Mateo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f1f48",
   "metadata": {
    "id": "fa0f1f48"
   },
   "source": [
    "### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd71c85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "4bd71c85",
    "outputId": "b9bba931-2b40-4e8f-9191-8fd01c7e6fbf"
   },
   "outputs": [],
   "source": [
    "corr_matrix = melted_df.corr()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Set the figure size to 12 inches by 12 inches\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)\n",
    "plt.title('Correlation Matrix', fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u4CXzfESfNbC",
   "metadata": {
    "id": "u4CXzfESfNbC"
   },
   "source": [
    "\n",
    "From the heat map, we can observe that most of the features exhibit weak relationships with each other, except for ROI and CV, which display a strong relationship."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
